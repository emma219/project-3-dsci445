---
title: "Housing Prices Analysis"
author: "Mandey Brown, Megan Dunnahoo, Emma Hamilton"
output: beamer_presentation
---

```{r setup, results='hide', warning=FALSE, message=FALSE}
library(knitr)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(naniar)
library(GGally)
library(tree)
library(glmnet)
library(rpart)
library(gbm)
library(randomForest)
library(pls)

```

```{r}
#reproducibility
set.seed(445)
```

# Motivation
	Housing prices play a central role in the U.S. economy. According to a *Congressional Research Service* article, *Introduction to U.S. Economy: Housing Market*, “at the individual level, roughly 65% of occupied housing units are owner occupied, homes are a substantial source of household wealth in the United States… housing accounts for a significant portion of all economic activity, and changes in the housing market can have broader effects on the economy.” The housing market is also incorporated into gross domestic product (GDP), which is considered the primary measure of economic activity for a country. Also, according to the article, *Introduction to U.S. Economy: Housing Market*, “as of 2020, spending on housing services was about $2.8 trillion, accounting for 13.3% of GDP. Taken together, spending within the housing market accounted for 17.5% of GDP in 2020.” The housing market not only affects the U.S. economy and GDP, up and coming college graduates will also soon be on the lookout for a place to live and knowing exactly what affects housing prices could prove to be incredibly useful.  

# Methodology 
	Housing prices influence the economy, but what influences house prices? Kaggle competition, *House Prices – Advanced Regression Techniques* provides a dataset of housing prices compiled by Dean De Cock in 2011, which describes the sale of individual residential property in Ames, Iowa from 2006 to 2010. The dataset includes 79 explanatory variables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous) all involved in evaluating home values. In order to find out which aspects of a home matter most when it comes to the final price of a home, the 79 available variables will be explored using a Decision Tree, Random Forest, PCR, Linear Regression, LASSO, and Ridge Regression models
 

```{r data}
test <- read.csv("/cloud/project/test.csv")
train <- read.csv("/cloud/project/train.csv")
```

Missing Values

```{r nas, warning=FALSE, message=FALSE}
#handle NA/missing values
#Approach:
  #convert NA to another level name for categorical 
  # or 0 for nominal 
  # dont remove any columns

#visualize number of NA per variable
gg_miss_var(train) + labs(y = "Number NA")

#convert these levels to characters (later be converted to levels)
#not continuous variables
train$MSSubClass <- as.character(train$MSSubClass)
train$OverallQual <- as.character(train$OverallQual)
train$OverallCond <- as.character(train$OverallCond)

#change the NA level in the categorical variables to NONE
train1 <- train %>%
 mutate_if(is.character, ~ fct_explicit_na(., na_level = "None"))

#change the NA level in the continuous variables to 0
train1[is.na(train1)] = 0

train1 <- as.data.frame(unclass(train1), stringsAsFactors = TRUE)
#sapply(train, class)
#cbind(lapply(lapply(train1, is.na), sum))

```

Exploratory Data Analysis

```{r explore, warning=FALSE, message=FALSE}
# See how time variables effect sale price
ggplot(train1, aes(x=YrSold, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Year Sold vs Sale Price") + xlab("Year Sold")

ggplot(train1, aes(x=MoSold, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Month Sold vs Sale Price") + xlab("Month Sold")

ggplot(train1, aes(x=YearBuilt, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Year Built vs Sale Price") + xlab("Year Built")

ggplot(train1, aes(x=YearRemodAdd, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Year Remodeled vs Sale Price") + xlab("Year Remodeled")

# Size indicator variables
df_size <- train1 %>% select(c(GrLivArea, TotalBsmtSF, BsmtFullBath, BsmtHalfBath, TotRmsAbvGrd, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, GarageArea, SalePrice))

ggpairs(df_size)

# Condition/quality variables
df_qual <- train1 %>% select(c(OverallQual, ExterQual, HeatingQC, KitchenQual, BsmtQual, GarageQual, SalePrice))

ggpairs(df_qual)

# Condition variables
df_cond <- train1 %>% select(c(OverallCond, ExterCond, BsmtCond, GarageCond, SalePrice))

ggpairs(df_cond)

```

Log Transform the Data

```{r}

```

Provide test set that contains Sale Price

```{r testSet}
# Split training set into new test and train sets so test set has SalePrice
n <- nrow(train1)
train_split <- seq_len(n) %in% sample(seq_len(n), round(0.7 * n))
train2 <- data.frame(train1[train_split,])
test2 <- data.frame(train1[-train_split,])

```

Decision Tree

```{r tree, collapse=TRUE}
# Create SalePrice tree
tree.SP <- tree(SalePrice ~., data = train2)

# Variables that get used:
print ("**** Below are variables used in the tree ****")
summary(tree.SP)$used

# Variables that are omitted in the tree
cat("\n")
print ("**** Below are omitted variables ****")
names(train2)[which(!(names(train2) %in%summary(tree.SP)$used))]

plot(tree.SP)
text(tree.SP, pretty=0)

```


Random Forest

```{r randomForest}
rf <- randomForest(SalePrice ~., train2, mtry=floor(sqrt(ncol(train2)-1)), importance=TRUE)

varImpPlot(rf, sort = TRUE, 
           n.var = 10, main = "Variables with most Predictive Power")

mean((predict(rf, test2) - test2$SalePrice)^2)

```


Boosting

```{r boost}
lambs <- seq(0.001, 0.05, length.out = 50)
length_lamb <- length(lambs)
tr_err <- rep(NA, length_lamb)
test_err <- rep(NA, length_lamb)

for(i in 1:length_lamb) {
 boost_hit <- gbm(SalePrice ~ ., data = train2, distribution = "gaussian", n.trees = 1000, shrinkage = lambs[i], verbose=F)
 tr_pred <- predict(boost_hit, train2, n.trees = 1000)
 test_pred <- predict(boost_hit, test2, n.trees = 1000)
 tr_err[i] <- mean((tr_pred - train2$SalePrice)^2)
 test_err[i] <- mean((test_pred - test2$SalePrice)^2)
}
plot(lambs, tr_err, type="b", xlab="Lambda", "ylab"="Training MSE")

plot(lambs, test_err, type="b", xlab="Lambda", "ylab"="Test MSE")
boost_fit <- gbm(SalePrice ~ ., data = train2, distribution = "gaussian", n.trees = 1000, shrinkage = lambs[which.min(test_err)])

summary(boost_fit)

```


LASSO or SVM?

```{r lasso}
trnmat <- model.matrix(SalePrice ~ . , data = train2)[, -1]
tstmat <- model.matrix(SalePrice ~ . , data = test2)[, -1]

lambda = 10 ^ seq(-2, 10, length.out = 100)

lasso <- glmnet(trnmat, train2$SalePrice, alpha = 1, folds = 10, lambda = lambda)
cv_lasso <- cv.glmnet(trnmat, train2$SalePrice, alpha = 1, folds = 10, lambda = lambda)
bestLambda <- cv_lasso$lambda.min
bestLambda

lasso.pred <- predict(lasso, newx = tstmat, s = bestLambda)
mean((test2$SalePrice - lasso.pred)^2)

cv_lasso$nzero[which.min(cv_lasso$cvm)]

predict(lasso, s=bestLambda, type="coefficients")

```



# References

“House Prices - Advanced Regression Techniques.” Kaggle, https://www.kaggle.com/c/house-prices-advanced-regression-techniques. 

Ames, Iowa: Alternative to the Boston Housing Data as an ... http://jse.amstat.org/v19n3/decock.pdf. 

“Convert Character to Factor in R: Vector, Data Frame Columns &amp; Variable.” Statistics Globe, 14 June 2021, https://statisticsglobe.com/convert-character-to-factor-in-r. 