---
title: "Housing Prices Analysis"
author: "Mandey Brown, Megan Dunnahoo, Emma Hamilton"
output: beamer_presentation
---

```{r setup, results='hide', warning=FALSE, message=FALSE}
library(knitr)
library(dplyr)
library(tidyr)
library(tidyverse)
library(ggplot2)
library(naniar)
library(GGally)
library(tree)
library(glmnet)
library(rpart)
library(gbm)
library(randomForest)
library(pls)

```

```{r}
#reproducibility
set.seed(445)
```

# Motivation
  Housing prices play a central role in the U.S. economy. According to a *Congressional Research Service* article, *Introduction to U.S. Economy: Housing Market*, “at the individual level, roughly 65% of occupied housing units are owner occupied, homes are a substantial source of household wealth in the United States… housing accounts for a significant portion of all economic activity, and changes in the housing market can have broader effects on the economy.” Buying a house is considered the most utilized and profitable investment for most of the population. The housing market is also incorporated into gross domestic product (GDP), which is considered the primary measure of economic activity for a country. Also, according to the article, *Introduction to U.S. Economy: Housing Market*, “as of 2020, spending on housing services was about $2.8 trillion, accounting for 13.3% of GDP. Taken together, spending within the housing market accounted for 17.5% of GDP in 2020.” 
	In addition to the majority of the population benefiting from predictions of housing prices, many professions and industries would benefit as well. Home appraisers, mortgage lenders, insurers, and tax assessors would be able to more accurately asses the value of a home. Housing price predictions would also prove invaluable for home builders. For this project, the co-owner of Deluxe Homes LLC was interviewed in order to gain more industry insight. The co-owner, Stu Sprecher emphasized the need for flexible pricing predictions that would enable home builders to maintain a profit margin while ordering materials and hiring contractors for each home build. The ability to customize house price predictions to a specific home could prove invaluable for him as an industry professional. 


# Methodology 
	Housing prices influence the economy, but what influences house prices? Kaggle competition, *House Prices – Advanced Regression Techniques* provides a dataset of housing prices compiled by Dean De Cock in 2011, which describes the sale of individual residential property in Ames, Iowa from 2006 to 2010. The dataset includes 79 explanatory variables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous) all involved in evaluating home values. In order to find out which aspects of a home matter most when it comes to the final price of a home, the 79 available variables will be explored using a Decision Tree, Random Forest, PCR, Linear Regression, LASSO, and Ridge Regression models
 

```{r data}
test <- read.csv("/cloud/project/test.csv")
train <- read.csv("/cloud/project/train.csv")
```

Missing Values

```{r nas, warning=FALSE, message=FALSE}
#handle NA/missing values
#Approach:
  #convert NA to another level name for categorical 
  # or 0 for nominal 
  # dont remove any columns

#visualize number of NA per variable
gg_miss_var(train) + labs(y = "Number NA")

#convert these levels to characters (later be converted to levels)
#not continuous variables
train$MSSubClass <- as.character(train$MSSubClass)
train$OverallQual <- as.character(train$OverallQual)
train$OverallCond <- as.character(train$OverallCond)

#change the NA level in the categorical variables to NONE
train1 <- train %>%
 mutate_if(is.character, ~ fct_explicit_na(., na_level = "None"))

#change the NA level in the continuous variables to 0
train1[is.na(train1)] = 0

train1 <- as.data.frame(unclass(train1), stringsAsFactors = TRUE)
#sapply(train, class)
#cbind(lapply(lapply(train1, is.na), sum))

```

Exploratory Data Analysis

```{r explore, warning=FALSE, message=FALSE}
# See how time variables effect sale price
ggplot(train1, aes(x=YrSold, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Year Sold vs Sale Price") + xlab("Year Sold")

ggplot(train1, aes(x=MoSold, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Month Sold vs Sale Price") + xlab("Month Sold")

ggplot(train1, aes(x=YearBuilt, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Year Built vs Sale Price") + xlab("Year Built")

ggplot(train1, aes(x=YearRemodAdd, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Year Remodeled vs Sale Price") + xlab("Year Remodeled")

# Size indicator variables
df_size <- train1 %>% select(c(GrLivArea, TotalBsmtSF, BsmtFullBath, BsmtHalfBath, TotRmsAbvGrd, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, GarageArea, SalePrice))
ggpairs(df_size)

# Quality variables
df_qual <- train1 %>% select(c(OverallQual, ExterQual, HeatingQC, KitchenQual, BsmtQual, GarageQual, SalePrice))
ggpairs(df_qual)

# Condition variables
df_cond <- train1 %>% select(c(OverallCond, ExterCond, BsmtCond, GarageCond, SalePrice))
ggpairs(df_cond)

```

Log Transform the Data

```{r, message=FALSE}
ggplot(data = train1, aes(SalePrice)) + geom_histogram(bins = 50)

ggplot(data = train1, aes(sample = SalePrice)) + 
  stat_qq() +
  stat_qq_line()

train1$SalePrice <- log(train1$SalePrice)

ggplot(data = train1, aes(SalePrice)) + geom_histogram(bins = 50)

ggplot(data = train1, aes(sample = SalePrice)) + 
  stat_qq() +
  stat_qq_line()

```

Provide test set that contains Sale Price

```{r testSet}
# Split training set into new test and train sets so test set has SalePrice
n <- nrow(train1)
train_split <- seq_len(n) %in% sample(seq_len(n), round(0.7 * n))
train2 <- data.frame(train1[train_split,])
test2 <- data.frame(train1[-train_split,])

```

Decision Tree

```{r tree, collapse=TRUE}
# Create SalePrice tree
tree.SP <- tree(SalePrice ~., data = train2)

# Variables that get used:
print ("**** Below are variables used in the tree ****")
summary(tree.SP)$used

# Variables that are omitted in the tree
cat("\n")
print ("**** Below are omitted variables ****")
names(train2)[which(!(names(train2) %in%summary(tree.SP)$used))]

plot(tree.SP)
text(tree.SP, pretty=0)

# pred <- predict(tree.SP, test2, type='class')
# confMat <- table(pred, test2$SalePrice)
# confMat

dt_MSE <- mean((predict(tree.SP, test2) - test2$SalePrice)^2)
dt_MSE

```


Random Forest

```{r randomForest}
rf <- randomForest(SalePrice ~., train2, mtry=floor(sqrt(ncol(train2)-1)), importance=TRUE)

varImpPlot(rf, sort = TRUE, 
           n.var = 10, main = "Variables with most Predictive Power")

rf_MSE <- mean((predict(rf, test2) - test2$SalePrice)^2)
rf_MSE

predict(rf, test)

```


Boosting

```{r boost, warning=FALSE}
lambs <- seq(0.001, 0.05, length.out = 50)
length_lamb <- length(lambs)
tr_err <- rep(NA, length_lamb)
test_err <- rep(NA, length_lamb)

for(i in 1:length_lamb) {
 boost_hit <- gbm(SalePrice ~ ., data = train2, distribution = "gaussian", n.trees = 1000, shrinkage = lambs[i], verbose=F)
 tr_pred <- predict(boost_hit, train2, n.trees = 1000)
 test_pred <- predict(boost_hit, test2, n.trees = 1000)
 tr_err[i] <- mean((tr_pred - train2$SalePrice)^2)
 test_err[i] <- mean((test_pred - test2$SalePrice)^2)
}
plot(lambs, tr_err, type="b", xlab="Lambda", "ylab"="Training MSE")

plot(lambs, test_err, type="b", xlab="Lambda", "ylab"="Test MSE")
boost_fit <- gbm(SalePrice ~ ., data = train2, distribution = "gaussian", n.trees = 1000, shrinkage = lambs[which.min(test_err)])

summary(boost_fit)

boost_MSE <- min(test_err)
boost_MSE

```


LASSO

```{r lasso}
trnmat<-model.matrix(SalePrice ~ ., data = train2)
tstmat<-model.matrix(SalePrice ~ ., data = test2)

lambda = 10 ^ seq(-2, 10, length.out = 100)

lasso.mod <- glmnet(trnmat, train2$SalePrice, alpha=0, lambda=lambda)

plot(lasso.mod)

cv.lasso <- cv.glmnet(trnmat, train2$SalePrice, alpha=0, lambda=lambda, folds = 10)

plot(cv.lasso)

bestlam.lasso <- cv.lasso$lambda.min
bestlam.lasso

best.lasso <- glmnet(trnmat, train2$SalePrice, alpha=0, lambda=bestlam.lasso)

pred.lasso <- predict(lasso.mod, s=bestlam.lasso, newx=tstmat)

lasso_MSE <- mean((test2$SalePrice - pred.lasso)^2)

```

Table MSE values

```{r}
MSE_table <- data.frame(Model = c("Decision Tree", "Random Forest", "Boosted Forest", "LASSO"), 
                        MSE = c(dt_MSE, rf_MSE, boost_MSE, lasso_MSE))

kable((MSE_table), caption = "MSE Values for Different Models", digits = 4)
```


Predict Sale Price using Random Forest
```{r}

```



# References

Ames, Iowa: Alternative to the Boston Housing Data as an ... http://jse.amstat.org/v19n3/decock.pdf. 

“Convert Character to Factor in R: Vector, Data Frame Columns &amp; Variable.” Statistics Globe, 14 June 2021, https://statisticsglobe.com/convert-character-to-factor-in-r. 

“House Prices - Advanced Regression Techniques.” Kaggle, https://www.kaggle.com/c/house-prices-advanced-regression-techniques.

Sprecher, Stu. “Deluxe Homes LLC Housing Prices.” 1 Dec. 2021. 