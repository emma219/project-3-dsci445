---
title: "Housing Prices Analysis"
author: "Mandey Brown, Megan Dunnahoo, Emma Hamilton"
output:
  word_document: default
  html_document: default
---

```{r setup, results='hide', warning=FALSE, message=FALSE}
library(knitr)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(GGally)
library(tree)
library(glmnet)
library(rpart)
library(gbm)
library(randomForest)
library(pls)

```

```{r}
#reproducibility
set.seed(445)
```

# Motivation
	Housing prices play a central role in the U.S. economy. According to a *Congressional Research Service* article, *Introduction to U.S. Economy: Housing Market*, “at the individual level, roughly 65% of occupied housing units are owner occupied, homes are a substantial source of household wealth in the United States… housing accounts for a significant portion of all economic activity, and changes in the housing market can have broader effects on the economy.” The housing market is also incorporated into gross domestic product (GDP), which is considered the primary measure of economic activity for a country. Also, according to the article, *Introduction to U.S. Economy: Housing Market*, “as of 2020, spending on housing services was about $2.8 trillion, accounting for 13.3% of GDP. Taken together, spending within the housing market accounted for 17.5% of GDP in 2020.” The housing market not only affects the U.S. economy and GDP, up and coming college graduates will also soon be on the lookout for a place to live and knowing exactly what affects housing prices could prove to be incredibly useful.  

# Methodology 
	Housing prices influence the economy, but what influences house prices? Kaggle competition, *House Prices – Advanced Regression Techniques* provides a dataset of housing prices compiled by Dean De Cock in 2011, which describes the sale of individual residential property in Ames, Iowa from 2006 to 2010. The dataset includes 79 explanatory variables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous) all involved in evaluating home values. In order to find out which aspects of a home matter most when it comes to the final price of a home, the 79 available variables will be explored using a Decision Tree, Random Forest, PCR, Linear Regression, LASSO, and Ridge Regression models
 

```{r data}
#test <- read.csv("/cloud/project/test.csv")
train <- read.csv("/cloud/project/train.csv")
```

Missing Values

```{r nas, warning=FALSE, message=FALSE}
#deal with NA/missing values
#Approach 1)
  #convert NA to mode/mean

#get sum of missing values per column
cbind(lapply(lapply(train, is.na), sum))

#get percentage missing values per column
mv <- train %>% summarize_all(funs(sum(is.na(.)) / length(.)))

#find which columns have a missing percentage higher then 45%
#and remove them
which(mv > 0.45)
#remove:  Alley, FireplaceQu, PoolQC, Fence, and MiscFeature
train <- train[-c(7, 58, 73:75)]

#replace the remaining values with mean/mode appropriately
train$LotFrontage[is.na(train$LotFrontage)] <- round(mean(train$LotFrontage, na.rm = TRUE))
train$LotFrontage <- as.integer(train$LotFrontage)
train$GarageYrBlt[is.na(train$GarageYrBlt)] <- round(mean(train$GarageYrBlt, na.rm = TRUE))
train$GarageYrBlt <- as.integer(train$GarageYrBlt)

calc_mode <- function(x){
  # List the distinct / unique values
  distinct_values <- unique(x)

  # Count the occurrence of each distinct value
  distinct_tabulate <- tabulate(match(x, distinct_values))

  # Return the value with the highest occurrence
  distinct_values[which.max(distinct_tabulate)]
}

train <- train %>%
  mutate(across(everything(), ~replace_na(.x, calc_mode(.x))))

```

```{r nas, warning=FALSE, message=FALSE}
#deal with NA/missing values
#Approach 2)
  #convert NA to another level name for categorical 
  # or 0 for nominal 
  # dont remove any columns

train1 <- train

#get sum of missing values per column
cbind(lapply(lapply(train1, is.na), sum))

train1$MSSubClass <- as.character(train1$MSSubClass)
train1$OverallQual <- as.character(train1$OverallQual)
train1$OverallCond <- as.character(train1$OverallCond)

#change the NA level in the categorical variables to NONE
train1 %>%
 mutate_if(is.character, ~ fct_explicit_na(., na_level = "None"))

#change the NA level in the continuous variables to 0
train1[is.na(train)] = 0

#cbind(lapply(lapply(train1, is.na), sum))

train1 <- as.data.frame(unclass(train1), stringsAsFactors = TRUE)
sapply(train1, class)
cbind(lapply(lapply(train1, is.na), sum))

```

Convert Types

```{r conv}
#sapply(train, class)
#convert integer type to numeric
#convert character to factor
train <- as.data.frame(unclass(train), stringsAsFactors = TRUE)
#sapply(train, class)
train$MSSubClass <- as.factor(train$MSSubClass)
train$OverallQual <- as.factor(train$OverallQual)
train$OverallCond <- as.factor(train$OverallCond)
#sapply(train, class)

```

Exploratory Data Analysis

```{r explore}
# See how time variables effect sale price
ggplot(train, aes(x=YrSold, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Year Sold vs Sale Price") + xlab("Year Sold")
ggplot(train, aes(x=MoSold, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Month Sold vs Sale Price") + xlab("Month Sold")
ggplot(train, aes(x=YearBuilt, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Year Built vs Sale Price") + xlab("Year Built")
ggplot(train, aes(x=YearRemodAdd, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Year Remodeled vs Sale Price") + xlab("Year Remodeled")

# Size indicator variables
df_size <- train %>% select(c(GrLivArea, TotalBsmtSF, BsmtFullBath, BsmtHalfBath, TotRmsAbvGrd, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, GarageArea, SalePrice))

ggpairs(df_size)

# Condition/quality variables
df_qual <- train %>% select(c(OverallQual, ExterQual, HeatingQC, KitchenQual, BsmtQual, GarageQual, SalePrice))

ggpairs(df_qual)

# Condition variables
df_cond <- train %>% select(c(OverallCond, ExterCond, BsmtCond, GarageCond, SalePrice))

ggpairs(df_cond)

```

Provide test set that contains Sale Price

```{r testSet}
# Split training set into new test and train sets so test set has SalePrice
n <- nrow(train1)
train_split <- seq_len(n) %in% sample(seq_len(n), round(0.7 * n))
train2 <- data.frame(train1[train_split,])
test2 <- data.frame(train1[-train_split,])

```

Decision Tree

```{r tree, collapse=TRUE}
# Create SalePrice tree
tree.SP <- tree(SalePrice ~., data = train2)

# Variables that get used:
print ("**** Below are variables used in the tree ****")
summary(tree.SP)$used

# Variables that are omitted in the tree
cat("\n")
print ("**** Below are omitted variables ****")
names(train2)[which(!(names(train2) %in%summary(tree.SP)$used))]

plot(tree.SP)
text(tree.SP, pretty=0)

```


Random Forest

```{r randomForest}
rf <- randomForest(SalePrice ~., train2, mtry=floor(sqrt(ncol(train2)-1)), importance=TRUE)

varImpPlot(rf, sort = TRUE, 
           n.var = 10, main = "Variables with most Predictive Power")

mean((predict(rf, test2) - test2$SalePrice)^2)

```


Boosting

```{r boost}
lambs <- seq(0.001, 0.05, length.out = 50)
length_lamb <- length(lambs)
tr_err <- rep(NA, length_lamb)
test_err <- rep(NA, length_lamb)

for(i in 1:length_lamb) {
 boost_hit <- gbm(SalePrice ~ ., data = train2, distribution = "gaussian", n.trees = 1000, shrinkage = lambs[i], verbose=F)
 tr_pred <- predict(boost_hit, train2, n.trees = 1000)
 tr_err[i] <- mean((tr_pred - train2$SalePrice)^2)
}

plot(lambs, tr_err, type="b", xlab="Lambda", ylab="Training MSE")

boost_fit <- gbm(SalePrice ~ ., data = train2, distribution = "gaussian", n.trees = 1000, shrinkage = lambs[which.min(tr_err)])
summary(boost_fit)

#lambs <- seq(0.01, 3.01, by=0.1)
lambs <- seq(0.001, 0.05, length.out = 50)
length_lamb <- length(lambs)
tr_err <- rep(NA, length_lamb)
test_err <- rep(NA, length_lamb)

for(i in 1:length_lamb) {
 boost_hit <- gbm(SalePrice ~ ., data = train2, distribution = "gaussian", n.trees = 1000, shrinkage = lambs[i], verbose=F)
 tr_pred <- predict(boost_hit, train2, n.trees = 1000)
 test_pred <- predict(boost_hit, test2, n.trees = 1000)
 tr_err[i] <- mean((tr_pred - train2$SalePrice)^2)
 test_err[i] <- mean((test_pred - test2$SalePrice)^2)
}
plot(lambs, tr_err, type="b", xlab="Lambda", "ylab"="Training MSE")

plot(lambs, test_err, type="b", xlab="Lambda", "ylab"="Test MSE")
boost_fit <- gbm(SalePrice ~ ., data = train2, distribution = "gaussian", n.trees = 1000, shrinkage = lambs[which.min(test_err)])

summary(boost_fit)


```


LASSO

```{r lasso}
trnmat <- model.matrix(SalePrice ~ . , data = train2)[, -1]
tstmat <- model.matrix(SalePrice ~ . , data = test2)[, -1]

lambda = 10 ^ seq(-2, 10, length.out = 100)

lasso <- glmnet(trnmat, train2$SalePrice, alpha = 1, folds = 10, lambda = lambda)
cv_lasso <- cv.glmnet(trnmat, train2$SalePrice, alpha = 1, folds = 10, lambda = lambda)
bestLambda <- cv_lasso$lambda.min
bestLambda

lasso.pred <- predict(lasso, newx = tstmat, s = bestLambda)
mean((test2$SalePrice - lasso.pred)^2)

cv_lasso$nzero[which.min(cv_lasso$cvm)]

predict(lasso, s=bestLambda, type="coefficients")

```



# References

“House Prices - Advanced Regression Techniques.” Kaggle, https://www.kaggle.com/c/house-prices-advanced-regression-techniques. 

Ames, Iowa: Alternative to the Boston Housing Data as an ... http://jse.amstat.org/v19n3/decock.pdf. 

“Convert Character to Factor in R: Vector, Data Frame Columns &amp; Variable.” Statistics Globe, 14 June 2021, https://statisticsglobe.com/convert-character-to-factor-in-r. 