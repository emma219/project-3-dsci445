---
title: "Housing Prices Analysis"
author: "Mandey Brown, Megan Dunnahoo, Emma Hamilton"
output: beamer_presentation
---

```{r setup, results='hide', warning=FALSE, message=FALSE}
library(knitr)
library(dplyr)
library(tidyr)
library(tidyverse)
library(ggplot2)
library(naniar)
library(GGally)
library(tree)
library(glmnet)
library(rpart)
library(gbm)
library(randomForest)
library(pls)
library(e1071)

```

```{r}
#reproducibility
set.seed(445)
```

# Motivation
  Housing prices play a central role in the U.S. economy. According to a *Congressional Research Service* article, *Introduction to U.S. Economy: Housing Market*, “at the individual level, roughly 65% of occupied housing units are owner occupied, homes are a substantial source of household wealth in the United States… housing accounts for a significant portion of all economic activity, and changes in the housing market can have broader effects on the economy.” Buying a house is considered the most utilized and profitable investment for most of the population. The housing market is also incorporated into gross domestic product (GDP), which is considered the primary measure of economic activity for a country. Also, according to the article, *Introduction to U.S. Economy: Housing Market*, “as of 2020, spending on housing services was about $2.8 trillion, accounting for 13.3% of GDP. Taken together, spending within the housing market accounted for 17.5% of GDP in 2020.” 
	In addition to the majority of the population benefiting from predictions of housing prices, many professions and industries would benefit as well. Home appraisers, mortgage lenders, insurers, and tax assessors would be able to more accurately asses the value of a home. Housing price predictions would also prove invaluable for home builders. For this project, the co-owner of Deluxe Homes LLC was interviewed in order to gain more industry insight. The co-owner, Stu Sprecher emphasized the need for flexible pricing predictions that would enable home builders to maintain a profit margin while ordering materials and hiring contractors for each home build. The ability to customize house price predictions to a specific home could prove invaluable for him as an industry professional. 


# Methodology 
	Built off Kaggle competition, *House Prices – Advanced Regression Techniques,* this project utilizes housing prices compiled by Dean De Cock in 2011, which describes the sale of individual residential property in Ames, Iowa from 2006 to 2010. The dataset includes 79 explanatory variables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous) all involved in evaluating home values. In order to build predictive models for housing price, an exploratory analysis was conducted followed by preprocessing. The project focuses on advanced regression models including a Decision Tree, Random Forest, Bagging, LASSO, and out of curiosity, Boosting. \
	A Decision Tree model is first explored as the output is easily interpreted and its graphical representation can be straightforwardly related to the predicting housing price. Though the downfalls of Decision Trees are known, such as overfitting. Instead of taking the approach of pre-pruning the Decision Tree using Chi^2 test, which is “an algorithm used to find out the statistical significance between parent and child nodes” (Analytics Vidhya), or post-pruning using error estimation, in order to avoid overfitting and get a better understanding of the significance of the data’s variables, other models were explored, including Random Forest.  \
	A Random Forest model was also fit to the dataset as it is able to handle large datasets containing higher dimensionality, which was thought to be an appropriate choice as the Ames housing dataset contains 79 variables. A Random Forest model was also chosen as it is able to “reduce correlation between trees by injecting more randomness into the tree-growing process” (Greenwell et al). It was also chosen as a means of identifying which of the 79 variables were significant while predicting house price. \
	In addition to a Random Forest model, a Bagging model was also fit to the dataset as it was thought that a Bagging model’s methods of using collections of training data subsets to train multiple decision trees, of which the average would be used, would not only help avoid overfitting the data, but provide a more robust prediction of housing prices than a single Decision Tree model. \
	As feature selection is a large component of this project, it was thought that a LASSO model would also prove useful for reducing dimensionality. A LASSO model was thought to offer high prediction accuracy for this dataset since the model’s method includes shrinking the regression coefficients (some of them to zero), while also reducing variance and minimizing bias.\
	An AdaBoost Boosting model was also fit as it was thought that a Boosting model may increase predictive accuracy of housing prices via its ability add strength or weight to specific classifiers after taking into account the previous classifier’s success. It was thought that a Boosting model would help reduce dimensionality by resulting in significant classifiers being assigned higher weights than less significant classifiers. However, it is believed that a Bagging model will perform better with this dataset than the chosen AdaBoost Boosting model, as Boosting does not help avoid over-fitting as a Bagging model does.  \


```{r data}
test <- read.csv("/cloud/project/test.csv")
train <- read.csv("/cloud/project/train.csv")
```

# Missing Values

After importing the testing and training data from the Kaggle repository, an initial analysis of missing values was conducted:

```{r nas, warning=FALSE, message=FALSE}
#handle NA/missing values
#Approach:
  #convert NA to another level name for categorical 
  # or 0 for nominal 
  # dont remove any columns

#visualize number of NA per variable
train_na <- train[,which(colSums(is.na(train)) > 0)]
gg_miss_var(train) + labs(y = "Number NA")

#convert these levels to characters (later be converted to levels)
#not continuous variables
train$MSSubClass <- as.character(train$MSSubClass)
train$OverallQual <- as.character(train$OverallQual)
train$OverallCond <- as.character(train$OverallCond)

#Only 1 NA in Electrial --> replace with Mode
#Also impractical to not have electricity

#function to calculate the mode
getmode <- function(n) {
   uniq <- unique(n)
   uniq[which.max(tabulate(match(n, uniq)))]
}

mode_Electrical <- getmode(train$Electrical)
train$Electrical[is.na(train$Electrical)] <- mode_Electrical

#change the NA level in the categorical variables to NONE
train1 <- train %>%
 mutate_if(is.character, ~ fct_explicit_na(., na_level = "None"))

#change the NA level in the continuous variables to 0
train1[is.na(train1)] = 0

train1 <- as.data.frame(unclass(train1), stringsAsFactors = TRUE)
#sapply(train, class)
#cbind(lapply(lapply(train1, is.na), sum))

```


It was found that the data for the following variables contained the most Missing Values:

PoolQC       > 1250 (>85%) Missing Values \
MiscFeature  > 1250 (>85%) Missing Values \
Alley        > 1250 (>85%) Missing Values \
Fence        > 1000 (>68%) Missing Values \
FireplaceQu  > 500  (>34%) Missing Values \
LotFrontface > 250  (>17%) Missing Values \

In order to handle the NA/Missing Values, the NA level in the categorical variables were changed to none as these NA values could not be imputed by using the mean, mode, or interpolation of the feature. This was because it was impossible and impractical to impute the value for the quality of a home's pool (PoolQC), when the home did not come with a pool. 

In order to handle the NA/Missing Values of continuous variables, all NA values were converted to zero. This made intuitive sense as as it did not make sense to impute any values other than zero for continuous variables such as MasVnrArea or masonry veneer area in square feet if a home did not contain any masonry veneer area. 


# Exploratory Data Analysis

In order to better understand the data and gain insight into the relationship between variables, an explanatory data analysis was performed. 

```{r explore, warning=FALSE, message=FALSE}
# See how time variables effect sale price
ggplot(train1, aes(x=YrSold, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Year Sold vs Sale Price") + xlab("Year Sold")

ggplot(train1, aes(x=MoSold, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Month Sold vs Sale Price") + xlab("Month Sold")

ggplot(train1, aes(x=YearBuilt, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Year Built vs Sale Price") + xlab("Year Built")

ggplot(train1, aes(x=YearRemodAdd, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Year Remodeled vs Sale Price") + xlab("Year Remodeled")

# Size indicator variables
df_size <- train1 %>% select(c(GrLivArea, TotalBsmtSF, BsmtFullBath, BsmtHalfBath, TotRmsAbvGrd, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, GarageArea, SalePrice))
ggpairs(df_size)
ggcorr(df_size, method = c("everything", "pearson"))

# Quality variables
df_qual <- train1 %>% select(c(OverallQual, ExterQual, HeatingQC, KitchenQual, BsmtQual, GarageQual, SalePrice))
ggpairs(df_qual)

# Condition variables
df_cond <- train1 %>% select(c(OverallCond, ExterCond, BsmtCond, GarageCond, SalePrice))
ggpairs(df_cond)
```

The first relationship explored in the Exploratory Data Analysis was the relationship between variables which involve time and SalePrice. These relationships were plotted as:

Sale Price vs. Year Sold \
Sale Price vs. Month Sold \
Sale Price vs. Year Built \
Sale Price vs. Year Remodeled \

The rather flat relationship between Sale Price and Year Sold proved interesting as it was thought that this relationship would show to be predominantly positive. It's been hypothesized that the housing market crash of 2007 might have affected this relationship. 

The relationship between Sale Price and Month Sold was expected, with the exception of sale prices in January. It was expected that homes would sell for more money in the summer months, but it was not expected that January would prove to have the highest sale price of all months.  

The relationship between Sale Price and Year Built was expected with a slightly positive relationship shown. 

The relationship between Sale Price and Year Remodeled also was expected with a slightly positive relationship shown.


The second part of the Exploratory Data Analysis was to create multiple scatter plot matrices for indicator variables, quality variables, and condition variables. Here a predominantly positive relationship between Sale Price and the indicator variables was found. However, what stood out most was the skewed distribution of the Sale Price variable across all three scatterplot matrices. Upon further analysis it was discovered that the SalePrice variable should be log transformed.  


# Log Transform the Data

The SalePrice variable was log transformed due to having a skewed distribution first discovered in the Exploratory Data Analysis. 

```{r, message=FALSE}
ggplot(data = train1, aes(SalePrice)) + geom_histogram(bins = 50)

ggplot(data = train1, aes(sample = SalePrice)) + 
  stat_qq() +
  stat_qq_line()

```


Further analysis of the SalePrice variable showed a non-normal right-skewed distribution in the above histogram. This non-normal distribution is also evident in the above (Q-Q) plot where the observations curve off of the line indicating the distribution is skewed. 


```{r, message=FALSE}

train1$SalePrice <- log(train1$SalePrice)

ggplot(data = train1, aes(SalePrice)) + geom_histogram(bins = 50)

ggplot(data = train1, aes(sample = SalePrice)) + 
  stat_qq() +
  stat_qq_line()

```


After log-transforming the SalePrice variable we now see a normal bell-curve shape in the distribution in the above histogram. The above (Q-Q) plot now shows the observations sticking closely to the line without any curvature away from the line. 


# Provide test set that contains Sale Price

Upon further analysis, it was discovered that an aspect of the Kaggle Competition was that the test dataset did not contain a SalePrice column as this is the condition for ranking the effectiveness of the predictive models submitted. Therefore, the training dataset was split into new test and training sets in order to analyze the data. 

```{r testSet}
# Split training set into new test and train sets so test set has SalePrice
n <- nrow(train1)
train_split <- seq_len(n) %in% sample(seq_len(n), round(0.7 * n))
train2 <- data.frame(train1[train_split,])
test2 <- data.frame(train1[-train_split,])

```

# Decision Tree

	A Decision Tree model was first explored as was thought that the output would be easily interpreted and its graphical representations would be straightforwardly related to predicting Sale Price. \
	In order to fit the Decision Tree, a SalePrice tree was first created. From that tree, the variables used were listed along with omitted variables. The tree was subsequently plotted and the test MSE of the tree was calculated. The test MSE of the Decision Tree was later compared with the test MSE of the other chosen models in order to choose the most accurate model for predicting Sale Price.   

```{r tree, collapse=TRUE}
# Create SalePrice tree
tree.SP <- tree(SalePrice ~., data = train2)

# Variables that get used:
print ("**** Below are variables used in the tree ****")
summary(tree.SP)$used

# Variables that are omitted in the tree
cat("\n")
print ("**** Below are omitted variables ****")
names(train2)[which(!(names(train2) %in%summary(tree.SP)$used))]

plot(tree.SP)
text(tree.SP, pretty=0)

# pred <- predict(tree.SP, test2, type='class')
# confMat <- table(pred, test2$SalePrice)
# confMat

dt_MSE <- mean((predict(tree.SP, test2) - test2$SalePrice)^2)
print(paste("Decision Tree Test MSE = ", dt_MSE))

```


# Random Forest
	A Random Forest model was also fit as it was believed that it would be able to handle large datasets containing higher dimensionality, which was thought to be an appropriate choice as the Ames housing dataset contains 79 variables. A Random Forest model was also chosen as it is able to “reduce correlation between trees by injecting more randomness into the tree-growing process” (Greenwell et al). It was also chosen as a means of identifying which of the 79 variables were significant while predicting house price. \
	After fitting the Random Forest model, the variables with the most predictive power were found. The test MSE of the Random Forest model was then calculated. The test MSE of the Random Forest model was later compared with the test MSE of the other chosen models in order to choose the most accurate model for predicting Sale Price. \  

```{r randomForest}
rf <- randomForest(SalePrice ~., train2, mtry=floor(sqrt(ncol(train2)-1)), importance=TRUE)

varImpPlot(rf, sort = TRUE, 
           n.var = 10, main = "Variables with most Predictive Power")

rf_MSE <- mean((predict(rf, test2) - test2$SalePrice)^2)
print(paste("Random Forest Test MSE = ", rf_MSE))

```

# Bagging

```{r bag}
# 1. Perform bagging on your training
bag_fit <- randomForest(SalePrice ~ ., data = train2, mtry = ncol(train2) - 1, importance = TRUE)

# 2. Make a plot of the importance values for each predictor
# data.frame(bag_fit$importance )%>%
#   mutate(variable = rownames(bag_fit$importance)) %>%
#   mutate(variable = factor(variable, levels = variable[order(MeanDecreaseGini)])) %>% ## trick to plot variable by descending Gini
#   ggplot() +
#   geom_point(aes(MeanDecreaseGini, variable))

varImpPlot(bag_fit, sort = TRUE, 
           n.var = 10, main = "Variables with most Predictive Power")

bag_MSE <- mean((predict(bag_fit, test2) - test2$SalePrice)^2)
bag_MSE

# # 3. Estimate the test error rate using your bagged tree model.
# confusion_bag <- table(pred = predict(bag_fit, test2, type = "class"), true = test2$SalePrice)
# 
# ## test error rate
# (confusion_bag[1, 2] + confusion_bag[2, 1])/sum(confusion_bag)
# 
# confusion_bag
```

# LASSO

	As feature selection was thought to be a large component of the project, a LASSO model was chosen as it offers high prediction accuracy as aids with high dimensionality by shrinking the regression coefficients (some of them to zero).\
	In order to fit a LASSO model, first the training and testing split data were transformed into matrices and lambda values were added. The coefficients were plotted along with test MSE at different lambta values. The best test MSE of the LASSO model was then calculated. The test MSE of the LASSO model was later compared with the test MSE of the other chosen models in order to choose the most accurate model for predicting Sale Price. \ 
	
```{r lasso}
trnmat<-model.matrix(SalePrice ~ ., data = train2)
tstmat<-model.matrix(SalePrice ~ ., data = test2)

lambda = 10 ^ seq(-2, 10, length.out = 100)

lasso.mod <- glmnet(trnmat, train2$SalePrice, alpha=0, lambda=lambda)

plot(lasso.mod)

cv.lasso <- cv.glmnet(trnmat, train2$SalePrice, alpha=0, lambda=lambda, folds = 10)

plot(cv.lasso)

bestlam.lasso <- cv.lasso$lambda.min
print(paste("LASSO Test MSE = ", bestlam.lasso))

best.lasso <- glmnet(trnmat, train2$SalePrice, alpha=0, lambda=bestlam.lasso)

pred.lasso <- predict(lasso.mod, s=bestlam.lasso, newx=tstmat)

lasso_MSE <- mean((test2$SalePrice - pred.lasso)^2)

```

# Boosting

```{r boost, warning=FALSE}
lambs <- seq(0.001, 0.05, length.out = 50)
length_lamb <- length(lambs)
tr_err <- rep(NA, length_lamb)
test_err <- rep(NA, length_lamb)

for(i in 1:length_lamb) {
 boost_hit <- gbm(SalePrice ~ ., data = train2, distribution = "gaussian", n.trees = 1000, shrinkage = lambs[i], verbose=F)
 tr_pred <- predict(boost_hit, train2, n.trees = 1000)
 test_pred <- predict(boost_hit, test2, n.trees = 1000)
 tr_err[i] <- mean((tr_pred - train2$SalePrice)^2)
 test_err[i] <- mean((test_pred - test2$SalePrice)^2)
}
plot(lambs, tr_err, type="b", xlab="Lambda", "ylab"="Training MSE")

plot(lambs, test_err, type="b", xlab="Lambda", "ylab"="Test MSE")
boost_fit <- gbm(SalePrice ~ ., data = train2, distribution = "gaussian", n.trees = 1000, shrinkage = lambs[which.min(test_err)])

summary(boost_fit)

boost_MSE <- min(test_err)
boost_MSE

```


# Table of Test MSE Values

In order to compare the test MSE values of the chosen models, a table was created. The table showed the Random Forest model to be the best fit model for predicting Sale Price from the Housing dataset as it showed the lowest MSE of the models chosen. 

```{r}
MSE_table <- data.frame(Model = c("Decision Tree", "Random Forest", "Bagged Forest", "Boosted Forest", "LASSO"), 
                        MSE = c(dt_MSE, rf_MSE, bag_MSE, boost_MSE, lasso_MSE))

kable((MSE_table), caption = "MSE Values for Different Models", digits = 4)
```

Predict Sale Price using Random Forest

```{r testdata, echo=FALSE, warning=FALSE, message=FALSE}
#first get test data similar to training data
#visualize number of NA per variable
#gg_miss_var(test) + labs(y = "Number NA")
test_na <- test[,which(colSums(is.na(test)) > 0)]
gg_miss_var(test_na) + labs(y="Number NA")

#convert these levels to characters (later be converted to levels)
#not continuous variables
test$MSSubClass <- as.character(test$MSSubClass)
test$OverallQual <- as.character(test$OverallQual)
test$OverallCond <- as.character(test$OverallCond)

#NA/None in the following variables is not practical
#replace with mode
#MSZoning
#Utilities
#Exterior1st
#Exterior2nd
#KitchenQual
#Functional
#SaleType

#function to calculate the mode
getmode <- function(n) {
   uniq <- unique(n)
   uniq[which.max(tabulate(match(n, uniq)))]
}

mode_MSZonzing <- getmode(test$MSZoning)
test$MSZoning[is.na(test$MSZoning)] <- mode_MSZonzing
mode_Utilities <- getmode(test$Utilities)
test$Utilities[is.na(test$Utilities)] <- mode_Utilities
mode_Exterior1st <- getmode(test$Exterior1st)
test$Exterior1st[is.na(test$Exterior1st)] <- mode_Exterior1st
mode_Exterior2nd <- getmode(test$Exterior2nd)
test$Exterior2nd[is.na(test$Exterior2nd)] <- mode_Exterior2nd
mode_KitchenQual <- getmode(test$KitchenQual)
test$KitchenQual[is.na(test$KitchenQual)] <- mode_KitchenQual
mode_Functional <- getmode(test$Functional)
test$Functional[is.na(test$Functional)] <- mode_Functional
mode_SaleType <- getmode(test$SaleType)
test$SaleType[is.na(test$SaleType)] <- mode_SaleType

#other factor levels convert NA to None
test1 <- test %>%
 mutate_if(is.character, ~ fct_explicit_na(., na_level = "None")) 

#any integer levels convert NA to 0
test1[is.na(test1)] = 0

test1 <- as.data.frame(unclass(test1), stringsAsFactors = TRUE)

#convert all numeric to integer (match training data classes)
test1$LotFrontage <- as.integer(test1$LotFrontage)
test1$MasVnrArea <- as.integer(test1$MasVnrArea)
test1$GarageYrBlt <- as.integer(test1$GarageYrBlt)
test1$BsmtFinSF1 <- as.integer(test1$BsmtFinSF1)
test1$BsmtFinSF2  <- as.integer(test1$BsmtFinSF2 )
test1$BsmtUnfSF <- as.integer(test1$BsmtUnfSF)
test1$TotalBsmtSF <- as.integer(test1$TotalBsmtSF)
test1$BsmtFullBath <- as.integer(test1$BsmtFullBath)
test1$BsmtHalfBath <- as.integer(test1$BsmtHalfBath)
test1$GarageCars  <- as.integer(test1$GarageCars)
test1$GarageArea <- as.integer(test1$GarageArea)

#sapply(test1, class)
#cbind(lapply(lapply(test1, is.na), sum))

```

```{r newlevels}
#Remove rows that contain levels in test that are not in train
#cant perform any prediction if this is the case

#MSSubClass --> 150
#only one instance --> remove that row

test1 %>% 
  group_by(MSSubClass) %>%
  summarise(no_rows = length(MSSubClass))

test3 <- droplevels(test1[!test1$MSSubClass == "150",])

```

```{r predRF}
#predict(bag_fit, test3)
```





# References

Ames, Iowa: Alternative to the Boston Housing Data as an ... http://jse.amstat.org/v19n3/decock.pdf. 

“Convert Character to Factor in R: Vector, Data Frame Columns &amp; Variable.” Statistics Globe, 14 June 2021, https://statisticsglobe.com/convert-character-to-factor-in-r. 

Greenwell, Bradley Boehmke & Brandon. “Hands-on Machine Learning with R.” Chapter 11 Random Forests, 1 Feb. 2020, https://bradleyboehmke.github.io/HOML/random-forest.html#fn29. 

Holtz, Yan. “Correlation Matrix with GGALLY.” – The R Graph Gallery, https://www.r-graph-gallery.com/199-correlation-matrix-with-ggally.html#:~:text=The%20ggpairs()%20function%20of,is%20displayed%20on%20the%20right. 

“House Prices - Advanced Regression Techniques.” Kaggle, https://www.kaggle.com/c/house-prices-advanced-regression-techniques.

Sprecher, Stu. “Deluxe Homes LLC Housing Prices.” 1 Dec. 2021.

“Tree Based Algorithms: Implementation in Python & R.” Analytics Vidhya, 26 Aug. 2021, https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-scratch-in-python/. 
