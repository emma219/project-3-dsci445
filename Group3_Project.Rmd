---
title: "Housing Prices Analysis"
author: "Mandey Brown, Megan Dunnahoo, Emma Hamilton"
output:
  pdf_document: default
---

```{r setup, results='hide', warning=FALSE, message=FALSE, echo=FALSE}
library(knitr)
library(dplyr)
library(tidyr)
library(tidyverse)
library(ggplot2)
library(naniar)
library(GGally)
library(tree)
library(glmnet)
library(rpart)
library(gbm)
library(randomForest)
library(pls)
library(e1071)
library(formatR)
library(gridExtra)

```

```{r, echo=FALSE}
#reproducibility
set.seed(445)

knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80), tidy=TRUE, fig.width = 6, fig.height=4)
```

# Motivation
|   Housing prices play a central role in the U.S. economy. According to a *Congressional Research Service* article, *Introduction to U.S. Economy: Housing Market*, “at the individual level, roughly 65% of occupied housing units are owner occupied, homes are a substantial source of household wealth in the United States… housing accounts for a significant portion of all economic activity, and changes in the housing market can have broader effects on the economy.” Buying a house is considered the most utilized and profitable investment for most of the population. The housing market is also incorporated into gross domestic product (GDP), which is considered the primary measure of economic activity for a country. Also, according to the article, *Introduction to U.S. Economy: Housing Market*, “as of 2020, spending on housing services was about $2.8 trillion, accounting for 13.3% of GDP. Taken together, spending within the housing market accounted for 17.5% of GDP in 2020.”  
|   In addition to the majority of the population benefiting from predictions of housing prices, many professions and industries would benefit as well. Home appraisers, mortgage lenders, insurers, and tax assessors would be able to more accurately asses the value of a home. Housing price predictions would also prove invaluable for home builders. For this project, the co-owner of Deluxe Homes LLC was interviewed in order to gain more industry insight. The co-owner, Stu Sprecher emphasized the need for flexible pricing predictions that would enable home builders to maintain a profit margin while ordering materials and hiring contractors for each home built. The ability to customize house price predictions to a specific home could prove invaluable for him as an industry professional.

# Methodology
|   Built off the Kaggle competition, *House Prices - Advanced Regression Techniques,* this project utilizes housing prices compiled by Dean De Cock in 2011, which describes the sale of individual residential property in Ames, Iowa from 2006 to 2010. The data set includes 79 explanatory variables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous) all involved in evaluating home values. In order to build predictive models for housing price, an exploratory analysis was conducted followed by preprocessing. The project focuses on advanced regression models including a Decision Tree, Random Forest, Bagging, LASSO, and out of curiosity, Boosting.  
|   A Decision Tree model is first explored as the output is easily interpreted and its graphical representation can be straightforwardly related to the predicting housing price. Though the downfalls of Decision Trees are known, such as overfitting. Instead of taking the approach of pre-pruning the Decision Tree using $\chi^2$ test, which is “an algorithm used to find out the statistical significance between parent and child nodes” (Analytics Vidhya), or post-pruning using error estimation, in order to avoid overfitting and get a better understanding of the significance of the data’s variables, other models were explored, including Random Forest.  
|   A Random Forest model was also fit to the data set as it is able to handle large data sets containing higher dimensionality, which was thought to be an appropriate choice as the Ames housing data set contains 79 variables. A Random Forest model was also chosen as it is able to “reduce correlation between trees by injecting more randomness into the tree-growing process” (Greenwell et al). It was also chosen as a means of identifying which of the 79 variables were significant while predicting house price.  
|   In addition to a Random Forest model, a Bagging model was also fit to the data set as it was thought that a Bagging model’s methods of using collections of training data subsets to train multiple decision trees, of which the average would be used, would not only help avoid overfitting the data, but provide a more robust prediction of housing prices than a single Decision Tree model.  
|   As feature selection is a large component of this project, it was thought that a LASSO model would also prove useful for reducing dimensionality. A LASSO model was thought to offer high prediction accuracy for this data set since the model’s method includes shrinking the regression coefficients (some of them to zero), while also reducing variance and minimizing bias.  
|   An AdaBoost Boosting model was also fit as it was thought that a Boosting model may increase predictive accuracy of housing prices via its ability add strength or weight to specific classifiers after taking into account the previous classifier’s success. It was thought that a Boosting model would help reduce dimensionality by resulting in significant classifiers being assigned higher weights than less significant classifiers. However, it is believed that a Bagging model will perform better with this data set than the chosen AdaBoost Boosting model, as Boosting does not help avoid over-fitting as a Bagging model does.
	
	
```{r data, echo=FALSE}
test <- read.csv("/cloud/project/test.csv")
train <- read.csv("/cloud/project/train.csv")
```

# Missing Values

|   After importing the testing and training data from the Kaggle repository, an initial analysis of missing values was conducted:

```{r nas, warning=FALSE, message=FALSE, echo=FALSE, fig.height=4}
#handle NA/missing values
#Approach:
  #convert NA to another level name for categorical 
  # or 0 for nominal 
  # don't remove any columns

#visualize number of NA per variable
train_na <- train[,which(colSums(is.na(train)) > 0)]
gg_miss_var(train_na) + labs(y = "Number NA") + ggtitle("Variables With One or More NA Values")

#convert these levels to characters (later be converted to levels)
#not continuous variables
train$MSSubClass <- as.character(train$MSSubClass)
train$OverallQual <- as.character(train$OverallQual)
train$OverallCond <- as.character(train$OverallCond)

#Only 1 NA in Electrical --> replace with Mode
#Also impractical to not have electricity

#function to calculate the mode
getmode <- function(n) {
   uniq <- unique(n)
   uniq[which.max(tabulate(match(n, uniq)))]
}

mode_Electrical <- getmode(train$Electrical)
train$Electrical[is.na(train$Electrical)] <- mode_Electrical

#change the NA level in the categorical variables to None
train1 <- train %>%
 mutate_if(is.character, ~ fct_explicit_na(., na_level = "None"))

#change the NA level in the continuous variables to 0
train1[is.na(train1)] = 0

train1 <- as.data.frame(unclass(train1), stringsAsFactors = TRUE)

#convert numeric classes to integer classes
for(col in colnames(train1)) {
  if(class(train1[, col]) == "numeric") {
    train1[, col] <- as.integer(train1[, col])
  }
}


orig_SalePrice <- train1$SalePrice
#sapply(train, class)
#cbind(lapply(lapply(train1, is.na), sum))

```


|   It was found that the data for the following variables contained the most Missing Values:

PoolQC       > 1250 (>85%) Missing Values \
MiscFeature  > 1250 (>85%) Missing Values \
Alley        > 1250 (>85%) Missing Values \
Fence        > 1000 (>68%) Missing Values \
FireplaceQu  > 500  (>34%) Missing Values \
LotFrontage > 250  (>17%) Missing Values \

|   In order to handle the NA/Missing Values, the NA level in the categorical variables were changed to 'None' as these NA values could not be imputed by using the mean, mode, or interpolation of the feature. This was because it was impossible and impractical to impute the value for the quality of a home's pool (PoolQC), when the home did not come with a pool. 

|   In order to handle the NA/Missing Values of continuous variables, all NA values were converted to zero. This made intuitive sense as as it did not make sense to impute any values other than zero for continuous variables such as MasVnrArea or masonry veneer area in square feet if a home did not contain any masonry veneer area. 


# Exploratory Data Analysis

|   In order to better understand the data and gain insight into the relationship between variables, an exploratory data analysis was performed. 

## Time variables

|   The first series of relationships explored in the Exploratory Data Analysis was the relationships between SalePrice and variables which involve time. These relationships were plotted as:

Sale Price vs. Year Sold \
Sale Price vs. Month Sold \
Sale Price vs. Year Built \
Sale Price vs. Year Remodeled \

```{r explore, warning=FALSE, message=FALSE, echo=FALSE}
# See how time variables effect sale price
p1 <- ggplot(train1, aes(x=YrSold, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Year Sold vs Sale Price") + xlab("Year Sold")

p2 <- ggplot(train1, aes(x=MoSold, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Month Sold vs Sale Price") + xlab("Month Sold")

p3 <- ggplot(train1, aes(x=YearBuilt, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Year Built vs Sale Price") + xlab("Year Built")

p4 <- ggplot(train1, aes(x=YearRemodAdd, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Year Remodeled vs Sale Price") + xlab("Year Remodeled")

grid.arrange(p1, p2, p3, p4, ncol=2)

```

|   The rather flat relationship between Sale Price and Year Sold proved interesting as it was thought that this relationship would show to be predominantly positive. It's been hypothesized that the housing market crash of 2007 might have affected this relationship. 

|   The relationship between Sale Price and Month Sold was expected, with the exception of sale prices in January. It was expected that homes would sell for more money in the summer months, but it was not expected that January would prove to have the highest sale price of all months.  

|   The relationship between Sale Price and Year Built was expected with a slightly positive relationship shown. 

|   The relationship between Sale Price and Year Remodeled also was expected with a slightly positive relationship shown.

## Other Variable Subsets

|   The second part of the Exploratory Data Analysis involved creating ggpairs plots, which shows an overview of relationships between variables. We did this on different subsets of variables indicating size, quality, and condition. We also created a correlation plot on the subset of size indicator variables. 

```{r explore2, warning=FALSE, message=FALSE, echo=FALSE, fig.width=3.5, fig.height=3}

# Size indicator variables
df_size <- train1 %>% select(c(GrLivArea, TotalBsmtSF, BsmtFullBath, BsmtHalfBath, TotRmsAbvGrd, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, GarageArea, SalePrice))
par(mfrow=c(1,2))
ggpairs(df_size, title="Size Indicator Variables")
ggcorr(df_size, method = c("everything", "pearson"))

# Quality variables
df_qual <- train1 %>% select(c(OverallQual, ExterQual, HeatingQC, KitchenQual, BsmtQual, GarageQual, SalePrice))

# Condition variables
df_cond <- train1 %>% select(c(OverallCond, ExterCond, BsmtCond, GarageCond, SalePrice))
par(mfrow=c(1,2))
ggpairs(df_qual, title="Quality Indicator Variables")
ggpairs(df_cond, title="Condition Indicator Variables")
```

|   Here, a predominantly positive relationship between Sale Price and the indicator variables was found. However, what stood out most was the skewed distribution of the Sale Price variable across all three scatterplot matrices. Upon further analysis, it was decided that the SalePrice variable should be log transformed.  

# Log Transform the Data

|   The SalePrice variable was log transformed due to having a skewed distribution first discovered in the Exploratory Data Analysis. 

```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.height=2}
p5 <- ggplot(data = train1, aes(SalePrice)) + geom_histogram(bins = 50)

p6 <- ggplot(data = train1, aes(sample = SalePrice)) + 
  stat_qq() +
  stat_qq_line()

grid.arrange(p5, p6, ncol=2)

```


|   Further analysis of the SalePrice variable showed a non-normal right-skewed distribution in the above histogram. This non-normal distribution is also evident in the above (Q-Q) plot where the observations curve off of the line indicating the distribution is skewed. 


```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.height=2}

train1$SalePrice <- log(train1$SalePrice)

p7 <- ggplot(data = train1, aes(SalePrice)) + geom_histogram(bins = 50)

p8 <- ggplot(data = train1, aes(sample = SalePrice)) + 
  stat_qq() +
  stat_qq_line()

grid.arrange(p7, p8, ncol=2)

```


|   After log-transforming the SalePrice variable, we now see a normal bell-curve shape in the distribution in the above histogram. The above (Q-Q) plot now shows the observations sticking closely to the line without any curvature away from the line. 


# Provide a test set that contains Sale Price

|   Upon further analysis, it was discovered that an aspect of the Kaggle Competition was that the test data set did not contain a SalePrice column as this is the condition for ranking the effectiveness of the predictive models submitted. Therefore, the training data set was split into new test and training sets in order to analyze the data and fit the models. This allowed us to produce test MSE values, which showed us the accuracy of the fitted models and gave us values to compare the models against each other. We decided on a 70/30 training/test split, which was selected randomly on the training data.  

```{r testSet, echo=FALSE}
# Split training set into new test and train sets so test set has SalePrice
n <- nrow(train1)
train_split <- seq_len(n) %in% sample(seq_len(n), round(0.7 * n))
train2 <- data.frame(train1[train_split,])
test2 <- data.frame(train1[-train_split,])
```

# Decision Tree

|   A Decision Tree model was first explored as it was thought that the output would be easily interpreted and its graphical representations would be straightforwardly related to predicting Sale Price.
| In order to fit the Decision Tree, a SalePrice tree was first created. From that tree, the variables used were listed along with omitted variables. The tree was subsequently plotted and the test MSE of the tree was calculated. The test MSE of the Decision Tree was later compared with the test MSE of the other chosen models in order to choose the most accurate model for predicting Sale Price.   

```{r tree, collapse=TRUE, echo=FALSE, fig.height=5}
# Create SalePrice tree
tree.SP <- tree(SalePrice ~., data = train2)

# Variables that get used:
print ("**** Below are variables used in the tree ****")
summary(tree.SP)$used

# Variables that are omitted in the tree
cat("\n")
print ("**** Below are omitted variables ****")
names(train2)[which(!(names(train2) %in%summary(tree.SP)$used))]

plot(tree.SP)
text(tree.SP, pretty=0)

dt_MSE <- mean((predict(tree.SP, test2) - test2$SalePrice)^2)
print(paste("Decision Tree Test MSE = ", dt_MSE))

```


# Random Forest
|   A Random Forest model was also fit as it was believed that it would be able to handle large data sets containing higher dimensionality, which was thought to be an appropriate choice as the Ames housing data set contains 79 variables. Additionally, this model was chosen as it is able to “reduce correlation between trees by injecting more randomness into the tree-growing process” (Greenwell et al), and act as a means of identifying which of the 79 variables were significant while predicting house sale price. 
|   After fitting the Random Forest model, the variables with the most predictive power were found. The test MSE of the Random Forest model was then calculated. The test MSE of the Random Forest model was later compared with the test MSE of the other chosen models in order to choose the most accurate model for predicting Sale Price. 

```{r randomForest, cache=TRUE, echo=FALSE}
rf <- randomForest(SalePrice ~., train2, mtry=floor(sqrt(ncol(train2)-1)), importance=TRUE)

varImpPlot(rf, sort = TRUE, 
           n.var = 10, main = "Variables with most Predictive Power")

rf_MSE <- mean((predict(rf, test2) - test2$SalePrice)^2)
print(paste("Random Forest Test MSE = ", rf_MSE))

```

# Bagging

|   Next, a Bagging model was fit to the data set as it was thought that a Bagging model’s methods of using collections of training data subsets to train multiple decision trees, of which the average would be used, would not only help avoid overfitting the data, but provide a more robust prediction of housing sale prices than a single Decision Tree model. The test MSE of the Bagging model was later compared with the test MSE of the other chosen models in order to choose the most accurate model for predicting Sale Price.  

```{r bag, cache=TRUE, echo=FALSE}
bag_fit <- randomForest(SalePrice ~ ., data = train2, mtry = ncol(train2) - 1, importance = TRUE)

varImpPlot(bag_fit, sort = TRUE, 
           n.var = 10, main = "Variables with most Predictive Power")

bag_MSE <- mean((predict(bag_fit, test2) - test2$SalePrice)^2)
bag_MSE

```

# LASSO

|   As feature selection was thought to be a large component of the project, a LASSO model was chosen as it offers high prediction accuracy and aids with high dimensionality by shrinking the regression coefficients (some of them to zero).   
|   In order to fit a LASSO model, first, the training and testing data were transformed into matrices and lambda values were added. The coefficients were plotted along with test MSE at different lambda values. The best test MSE of the LASSO model was then calculated. The test MSE of the LASSO model was later compared with the test MSE of the other chosen models in order to choose the most accurate model for predicting Sale Price.   
	
```{r lasso, cache=TRUE, echo=FALSE, fig.width=3.5}
trnmat<-model.matrix(SalePrice ~ ., data = train2)
tstmat<-model.matrix(SalePrice ~ ., data = test2)

lambda = 10 ^ seq(-2, 10, length.out = 100)

lasso.mod <- glmnet(trnmat, train2$SalePrice, alpha=0, lambda=lambda)

plot(lasso.mod)

cv.lasso <- cv.glmnet(trnmat, train2$SalePrice, alpha=0, lambda=lambda, folds = 10)

plot(cv.lasso)

bestlam.lasso <- cv.lasso$lambda.min
print(paste("LASSO Test MSE = ", bestlam.lasso))

best.lasso <- glmnet(trnmat, train2$SalePrice, alpha=0, lambda=bestlam.lasso)

pred.lasso <- predict(lasso.mod, s=bestlam.lasso, newx=tstmat)

lasso_MSE <- mean((test2$SalePrice - pred.lasso)^2)

```

# Boosting

|   An AdaBoost Boosting model was also fit as it was thought that a Boosting model may increase predictive accuracy of housing sale prices via its ability add strength or weight to specific classifiers after taking into account the previous classifier’s success. It was also thought that a Boosting model would help reduce dimensionality by resulting in significant classifiers being assigned higher weights than less significant classifiers. The test MSE of the AdaBoost Boosting model was later compared with the test MSE of the other chosen models in order to choose the most accurate model for predicting Sale Price.   

```{r boost, warning=FALSE, cache=TRUE, echo=FALSE, fig.width=3.5}
lambs <- seq(0.001, 0.05, length.out = 50)
length_lamb <- length(lambs)
tr_err <- rep(NA, length_lamb)
test_err <- rep(NA, length_lamb)

for(i in 1:length_lamb) {
 boost_hit <- gbm(SalePrice ~ ., data = train2, distribution = "gaussian", n.trees = 1000, shrinkage = lambs[i], verbose=F)
 tr_pred <- predict(boost_hit, train2, n.trees = 1000)
 test_pred <- predict(boost_hit, test2, n.trees = 1000)
 tr_err[i] <- mean((tr_pred - train2$SalePrice)^2)
 test_err[i] <- mean((test_pred - test2$SalePrice)^2)
}
plot(lambs, tr_err, type="b", xlab="Lambda", "ylab"="Training MSE")

plot(lambs, test_err, type="b", xlab="Lambda", "ylab"="Test MSE")

```

```{r boost2, warning=FALSE, cache=TRUE, echo=FALSE}
boost_fit <- gbm(SalePrice ~ ., data = train2, distribution = "gaussian", n.trees = 1000, shrinkage = lambs[which.min(test_err)])

summary(boost_fit)

boost_MSE <- min(test_err)
boost_MSE

```


# Table of Test MSE Values

|   In order to compare the test MSE values of the chosen models, a table was created. The table shows that the MSE for the Bagging and Random Forest models to be the smallest with Bagging having a slightly smaller MSE. However, the Random Forest model was ultimately selected as it has better interpretability when compared to Bagging, and the difference in MSE is not drastic. 

```{r, echo=FALSE}
MSE_table <- data.frame(Model = c("Decision Tree", "Random Forest", "Bagged Forest", "Boosted Forest", "LASSO"), 
                        MSE = c(dt_MSE, rf_MSE, bag_MSE, boost_MSE, lasso_MSE))

kable((MSE_table), caption = "Test MSE Values for Different Models", digits = 4)
```

# Predict Sale Price using Random Forest

|   The final step of the project was to take the test data supplied from Kaggle and use our best model, we decided to use the Random Forest model for the reasons previously explored, and predict the SalePrice using the model and the test data.
|   The data had to be configured similarly to that of the training data. The NA values needed to be addressed in the same way and in addition to that the levels of the training data needed to be present in the test data before prediction could even be done. 
```{r testdata, echo=FALSE, warning=FALSE, message=FALSE}
#first get test data similar to training data
#visualize number of NA per variable
test_na <- test[,which(colSums(is.na(test)) > 0)]
gg_miss_var(test_na) + labs(y="Number NA") + ggtitle("Variables With One or More NA Values")

#convert these levels to characters (later be converted to levels)
#not continuous variables
test$MSSubClass <- as.character(test$MSSubClass)
test$OverallQual <- as.character(test$OverallQual)
test$OverallCond <- as.character(test$OverallCond)

#NA/None in the following variables is not practical
#replace with mode
#MSZoning
#Utilities
#Exterior1st
#Exterior2nd
#KitchenQual
#Functional
#SaleType

#function to calculate the mode
getmode <- function(n) {
   uniq <- unique(n)
   uniq[which.max(tabulate(match(n, uniq)))]
}

mode_MSZonzing <- getmode(test$MSZoning)
test$MSZoning[is.na(test$MSZoning)] <- mode_MSZonzing
mode_Utilities <- getmode(test$Utilities)
test$Utilities[is.na(test$Utilities)] <- mode_Utilities
mode_Exterior1st <- getmode(test$Exterior1st)
test$Exterior1st[is.na(test$Exterior1st)] <- mode_Exterior1st
mode_Exterior2nd <- getmode(test$Exterior2nd)
test$Exterior2nd[is.na(test$Exterior2nd)] <- mode_Exterior2nd
mode_KitchenQual <- getmode(test$KitchenQual)
test$KitchenQual[is.na(test$KitchenQual)] <- mode_KitchenQual
mode_Functional <- getmode(test$Functional)
test$Functional[is.na(test$Functional)] <- mode_Functional
mode_SaleType <- getmode(test$SaleType)
test$SaleType[is.na(test$SaleType)] <- mode_SaleType

#other factor levels convert NA to None
test1 <- test %>%
 mutate_if(is.character, ~ fct_explicit_na(., na_level = "None")) 

#any integer levels convert NA to 0
test1[is.na(test1)] = 0

test1 <- as.data.frame(unclass(test1), stringsAsFactors = TRUE)

#convert all numeric to integer (match training data classes)
for(col in colnames(test1)) {
  if(class(test1[, col]) == "numeric") {
    test1[, col] <- as.integer(test1[, col])
  }
}

#sapply(test1, class)
#cbind(lapply(lapply(test1, is.na), sum))

```

```{r newlevels, echo=FALSE, include=FALSE}
#Remove rows that contain levels in test that are not in train
#cant perform any prediction if this is the case

#MSSubClass --> 150
#only one instance --> remove that row
test1 %>% 
  group_by(MSSubClass) %>%
  summarise(no_rows = length(MSSubClass))

#save Id and row number from where this was taken
#will need to add a value for this later
row_Sub_150 <- which(test1$MSSubClass == "150")
id_Sub_150 <- test1[row_Sub_150, "Id"]

test3 <- droplevels(test1[!test1$MSSubClass == "150",])

#str(train1)
#str(test3)

#make levels in train present in test
for(col in colnames(test3)) {
  if(class(test3[, col]) == "factor") {
    test3[, col] <- factor(test3[, col], levels = levels(train1[, col]))
  }
}

# test3$Exterior1st <- factor(test3$Exterior1st, levels = levels(train1$Exterior1st))
# identical(levels(test3$Exterior1st), levels(train1$Exterior1st))

```

|   Once the test data was properly configured, the Random Forest model was fit using the entire training set and the prediction of SalePrice using the configured test data was made.

```{r predRF, cache=TRUE, echo=FALSE}
#fit rf (chosen model of the ones used)
rf2 <- randomForest(SalePrice ~., train1, mtry=floor(sqrt(ncol(train1)-1)), importance=TRUE)

pred <- data.frame(predict(rf2, test3))

#take exp of sale price --> did log transformation previously
pred.sale <- exp(pred)

#make table with SalePrice and Id
names(pred.sale)[1] <- 'SalePrice'
pred.sale$Id <- test3$Id

pred2 <- pred.sale[c("Id", "SalePrice")]

#get mean SalePrice --> will add this to row where the MSSubClass of 150 was removed
mean_price <- mean(pred2$SalePrice)

```

```{r sub, echo=FALSE}
#add row for the MSSubClass row that was removed
r <- row_Sub_150
newrow <- cbind(id_Sub_150, mean_price)

insertRow <- function(existingDF, newrow, r) {
  existingDF[seq(r+1,nrow(existingDF)+1),] <- existingDF[seq(r,nrow(existingDF)),]
  existingDF[r,] <- newrow
  existingDF
}

pred3 <- insertRow(pred2, newrow, r)

```

# Results
|   In this project, five models were created and fit with training data. The top two models, Bagging and Random Forest, resulted in test MSE values of 0.0096 and 0.0097, respectively, on the portion of training data set aside for testing. The Random Forest model was selected to predict Sale Price values on the Kaggle test set. The predicted Sale Price values were submitted to Kaggle and a score of 0.14931 was given. According to the Kaggle competition, this score is calculated by taking the root mean squared error between the logarithm of the predicted value and the logarithm of the observed sales price. This corresponds to a place of 3074 out of 5396 submissions. However, most of the top 1000 submissions had RMSE scores of about 0.12, which isn't very far off from the score obtained using the Random Forest model created in this project. Overall, the selected model was proven to be fairly accurate at predicting Sale Price.   

# Outside Exploration
|   An original hope for the project was to see if this model and data could be applied outside of the original population. We worked with the co-owner of Deluxe Homes LLC, Stu Sprecher who is a licensed contractor to explore what we had been finding and get some industry insights. The first problem that was discovered was that all of the models were showing that predictors such as Neighborhood and OverallQuality were very significant in predicting sale price. This was an issue as for starters, the neighborhoods of Ames, Iowa cannot be easily compared to a neighborhoods in Berthoud, Colorado. Additionally, these quality ratings are very subjective, what Stu may rate an 8, Dean De Cock who compiled this data, might rate as a 6 for example. Stu also pointed out that this model and data fail to consider certain aspects involved in building a home that can impact sale price. An example he kept returning to was the soil quality. Where he has started to build in Severance, Colorado has much poorer soil quality then where he was building in Berthoud, Colorado. As such, it costs a lot for to reinforce the foundation to account for that poor soil quality and as such the price of the home will increase. Additionally, prices of building materials has shifted recently, making production of a home more costly which also impacts that sale price. 
|   Once it was concluded that fitting the model using data provided from Stu in the Berthoud, Colorado area would not be ideal, we considered finding similar data in Ames but more recent. Through further considerations that would also prove problematic as the housing market has shifted in the last decade and it is hard to determine if the significance of a predictor such as Neighborhood has also shifted. Ultimately, it appears that the Ames, Iowa housing data appears to simply be a snap shot in time. It can help predict sale prices for that area in that given time frame but with so many influences outside of the data it is hard to use this data, and the models that are fit with the data, to predict housing prices for other locations or even dates.

# References

Ames, Iowa: Alternative to the Boston Housing Data as an ... http://jse.amstat.org/v19n3/decock.pdf. 

“Convert Character to Factor in R: Vector, Data Frame Columns &amp; Variable.” Statistics Globe, 14 June 2021, https://statisticsglobe.com/convert-character-to-factor-in-r. 

Greenwell, Bradley Boehmke & Brandon. “Hands-on Machine Learning with R.” Chapter 11 Random Forests, 1 Feb. 2020, https://bradleyboehmke.github.io/HOML/random-forest.html#fn29. 

Holtz, Yan. “Correlation Matrix with GGALLY.” – The R Graph Gallery, https://www.r-graph-gallery.com/199-correlation-matrix-with-ggally.html#:~:text=The%20ggpairs()%20function%20of,is%20displayed%20on%20the%20right. 

“House Prices - Advanced Regression Techniques.” Kaggle, https://www.kaggle.com/c/house-prices-advanced-regression-techniques.

Sprecher, Stu. “Deluxe Homes LLC Housing Prices.” 1 Dec. 2021.

“Tree Based Algorithms: Implementation in Python & R.” Analytics Vidhya, 26 Aug. 2021, https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-scratch-in-python/. 
