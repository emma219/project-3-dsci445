---
title: "DSCI 445 Project Presentation"
author: "Megan Dunnahoo, Mandey Brown, Emma Hamilton"
output:
  beamer_presentation:
    theme: default
    colortheme: beaver
    fonttheme: structurebold
    incremental: true
  
---

```{r setup, results='hide', warning=FALSE, message=FALSE, include=FALSE}
library(knitr)
library(dplyr)
library(tidyr)
library(tidyverse)
library(ggplot2)
library(naniar)
library(GGally)
library(tree)
library(glmnet)
library(rpart)
library(gbm)
library(randomForest)
library(pls)
library(cowplot)

#reproducibility
set.seed(445)
```


# Motivation
\begin{itemize}
    \item[--] Housing prices affects U.S. economy
    \begin{itemize}
        \item[--] 65-percent of houses are owner occupied
        \item[--] Housing is a profitable investment
        \item[--] Housing Market accounts for 13.3% of GDP
        \end{itemize}
\end{itemize}
\begin{itemize}
    \item[--] Many professions and industries would benefit
    \begin{itemize}
        \item[--] Appraisers
        \item[--] Tax assessors
        \item[--] Mortgage lenders
        \item[--] Insurers
        \item[--] Home Builders
        \begin{itemize}
            \item[--] Worked with co-owner of Deluxe Homes LLC
            \item[--] Anticipate building costs
            \item[--] Need for flexible predictions based on specifics of home
        \end{itemize}
      \end{itemize}
\end{itemize}  

# Methodology
\begin{itemize}
    \item[--] Kaggle Knowledge Competition 
    \begin{itemize}
        \item[--] Dataset includes 79 variables
        \begin{itemize}
            \item[--] 23 ordinal
            \item[--] 14 discrete
            \item[--] 20 continuous
            \end{itemize}
        \item[--] Exploratory Analysis
        \item[--] Preprocessing
        \item[--] Advanced Regression Techniques
        \begin{itemize}
            \item[--] Decision Tree
            \item[--] Random Forest
            \item[--] LASSO
            \item[--] Boosting
            \end{itemize}
    \end{itemize}
\end{itemize}


```{r data, include=FALSE}
test <- read.csv("/cloud/project/test.csv")
train <- read.csv("/cloud/project/train.csv")
```

# Handling Missing Values
\small We visualized the number of missing values for each variable and produced a plot which shows the number of NA values for variables with at least 1 NA value.   

```{r nas, warning=FALSE, message=FALSE, echo=FALSE, fig.height=5}
#handle NA/missing values
#Approach:
  #convert NA to another level name for categorical 
  # or 0 for nominal 
  # dont remove any columns

#visualize number of NA per variable
#gg_miss_var(train) + labs(y = "Number NA")

#visualize variables with missing values
train_na <- train[,which(colSums(is.na(train)) > 0)]
gg_miss_var(train_na) + labs(y="Number NA")
```

# Missing Categorical/Character Variable Values
The categorical variables that have missing values are PoolQC, Fence, MiscFeature, Alley, FireplaceQu, GarageType, GarageFinish, GarageQual, GarageCond, BsmtQual, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtCond, MasVnrType, and Electrical. NAs for all of these variables, except for Electrical, likely represent the absence of a pool, fence, alley access, fireplace, garage, basement, etc. For these variables, we replaced the missing values with the level "None". For Electrical, there was only one missing value, which we replaced with the most common Electrical type.   

# Missing Numeric Variable Values
The numerical variables that have missing values are LotFrontage, GarageYrBuilt, and MasVnrArea. The missing values for these variables, similarly, likely mean that there is no garage, masonry veneer, or street connected to the property. Therefore, we replaced these missing values with 0.

```{r nas2, warning=FALSE, message=FALSE, include=FALSE}
#convert these levels to characters (later be converted to levels)
#not continuous variables
train$MSSubClass <- as.character(train$MSSubClass)
train$OverallQual <- as.character(train$OverallQual)
train$OverallCond <- as.character(train$OverallCond)

#change the NA level in the categorical variables to NONE
train1 <- train %>%
 mutate_if(is.character, ~ fct_explicit_na(., na_level = "None"))

#change the NA level in the continuous variables to 0
train1[is.na(train1)] = 0

train1 <- as.data.frame(unclass(train1), stringsAsFactors = TRUE)
#sapply(train, class)
#cbind(lapply(lapply(train1, is.na), sum))

```

# Exploratory Data Analysis
We first looked at some of the time variables vs Sale Price. We included red dots for the mean and blue dots for the median.

```{r explore, warning=FALSE, message=FALSE, echo=FALSE}
# See how time variables effect sale price
ggplot(train1, aes(x=YrSold, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Year Sold vs Sale Price") + xlab("Year Sold")
```

# Exploratory Data Analysis Cont.

```{r explore2, warning=FALSE, message=FALSE, echo=FALSE}
ggplot(train1, aes(x=MoSold, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Month Sold vs Sale Price") + xlab("Month Sold")
```

# Exploratory Data Analysis Cont.

```{r explore3, warning=FALSE, message=FALSE, echo=FALSE}
ggplot(train1, aes(x=YearBuilt, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Year Built vs Sale Price") + xlab("Year Built")
```

# Exploratory Data Analysis Cont.

```{r explore4, warning=FALSE, message=FALSE, echo=FALSE}
ggplot(train1, aes(x=YearRemodAdd, y=SalePrice)) +
  geom_point() +
  stat_summary(fun = "mean", geom = "point", color="red") +
  stat_summary(fun = "median", geom = "point", color="blue") +
  ggtitle("Year Remodeled vs Sale Price") + xlab("Year Remodeled")
```

# Exploratory Data Analysis Cont.

\small After this, we subsetted the variables into different general categories to make ggpair plots. This first plot shows variables which indicate size of the house. These include square feet of different living areas, number of bedrooms, number of bathrooms, etc.

```{r explore5, warning=FALSE, message=FALSE, echo=FALSE, fig.height=5}
# Size indicator variables
df_size <- train1 %>% select(c(GrLivArea, TotalBsmtSF, BsmtFullBath, BsmtHalfBath, TotRmsAbvGrd, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, GarageArea, SalePrice))
ggpairs(df_size)
```

# Exploratory Data Analysis Cont.
This plot shows variables which indicate the quality of the house.

```{r explore6, warning=FALSE, message=FALSE, echo=FALSE}

# Quality variables
df_qual <- train1 %>% select(c(OverallQual, ExterQual, HeatingQC, KitchenQual, BsmtQual, GarageQual, SalePrice))
ggpairs(df_qual)
```

# Exploratory Data Analysis Cont.
This plot shows variables which indicate the condition of the house.

```{r explore7, warning=FALSE, message=FALSE, echo=FALSE}
# Condition variables
df_cond <- train1 %>% select(c(OverallCond, ExterCond, BsmtCond, GarageCond, SalePrice))
ggpairs(df_cond)

```

# Log Transform the Data
We decided to log transform Sale Price as it violates the assumption of normality. These are the plots showing Sale Price before the transformation.

```{r log1, warning=FALSE, message=FALSE, echo=FALSE, fig.height=5}
plot1 <- ggplot(data = train1, aes(SalePrice)) + geom_histogram(bins = 50)

plot2 <- ggplot(data = train1, aes(sample = SalePrice)) + 
  stat_qq() +
  stat_qq_line()

plot_grid(plot1, plot2, labels = "AUTO")
```

# Log Transform the Data
These are the plots showing Sale Price after the log transformation. 

```{r log2, warning=FALSE, message=FALSE, echo=FALSE, fig.height=5}
train1$SalePrice <- log(train1$SalePrice)

plot3 <- ggplot(data = train1, aes(SalePrice)) + geom_histogram(bins = 50)

plot4 <- ggplot(data = train1, aes(sample = SalePrice)) + 
  stat_qq() +
  stat_qq_line()

plot_grid(plot3, plot4, labels = "AUTO")
```


```{r testSet, include=FALSE}
# Split training set into new test and train sets so test set has SalePrice
n <- nrow(train1)
train_split <- seq_len(n) %in% sample(seq_len(n), round(0.7 * n))
train2 <- data.frame(train1[train_split,])
test2 <- data.frame(train1[-train_split,])

```

# Tree-Based Methods

\begin{itemize}
    \item[--] Variety of Trees-Based Methods used
    \begin{itemize}
        \item[--] Curious which model might perform better
        \item[--] Allow for good prediction
        \begin{itemize}
            \item[--] Goal is to predict Sale Price
        \end{itemize}
        \item[--] Allow for ease of interpretation
        \item[--] Good visualization
        \item[--] Suggest which variables are most significant
    \end{itemize}
\end{itemize}

# Decision Tree
 
```{r tree, collapse=TRUE, echo=FALSE}
# Create SalePrice tree
tree.SP <- tree(SalePrice ~., data = train2)

# Variables that get used:
#print ("**** Below are variables used in the tree ****")
#summary(tree.SP)$used

# Variables that are omitted in the tree
#cat("\n")
#print ("**** Below are omitted variables ****")
#names(train2)[which(!(names(train2) %in%summary(tree.SP)$used))]

plot(tree.SP)
text(tree.SP, pretty=0)

# pred <- predict(tree.SP, test2, type='class')
# confMat <- table(pred, test2$SalePrice)
# confMat

dt_MSE <- mean((predict(tree.SP, test2) - test2$SalePrice)^2)

```


# Random Forest

```{r randomForest, echo=FALSE}
rf <- randomForest(SalePrice ~., train2, mtry=floor(sqrt(ncol(train2)-1)), importance=TRUE)

varImpPlot(rf, sort = TRUE, 
           n.var = 10, main = "Variables with most Predictive Power")

rf_MSE <- mean((predict(rf, test2) - test2$SalePrice)^2)

```

# Boosting

```{r boost, warning=FALSE, echo=FALSE}
lambs <- seq(0.001, 0.05, length.out = 50)
length_lamb <- length(lambs)
tr_err <- rep(NA, length_lamb)
test_err <- rep(NA, length_lamb)

for(i in 1:length_lamb) {
 boost_hit <- gbm(SalePrice ~ ., data = train2, distribution = "gaussian", n.trees = 1000, shrinkage = lambs[i], verbose=F)
 tr_pred <- predict(boost_hit, train2, n.trees = 1000)
 test_pred <- predict(boost_hit, test2, n.trees = 1000)
 tr_err[i] <- mean((tr_pred - train2$SalePrice)^2)
 test_err[i] <- mean((test_pred - test2$SalePrice)^2)
}
par(mfrow=c(1,2))
plot(lambs, tr_err, type="b", xlab="Lambda", "ylab"="Training MSE")
plot(lambs, test_err, type="b", xlab="Lambda", "ylab"="Test MSE")
```

# Boosting Cont. 

```{r boost2, warning=FALSE, echo=FALSE}
boost_fit <- gbm(SalePrice ~ ., data = train2, distribution = "gaussian", n.trees = 1000, shrinkage = lambs[which.min(test_err)])

res <- summary(boost_fit)
summary(boost_fit)

```

# Boosting Cont. 

```{r boost3, warning=FALSE, echo=FALSE}
res[1:15,]

boost_MSE <- min(test_err)

```

# Lasso

\begin{itemize}
    \item[--] Many predictor variables (72 total)
    \item[--] Desire to determine which subset to use
    \item[--] Allow for ease of interpretation
\end{itemize}

```{r lasso, echo=FALSE, fig.height=4}
trnmat<-model.matrix(SalePrice ~ ., data = train2)
tstmat<-model.matrix(SalePrice ~ ., data = test2)

lambda = 10 ^ seq(-2, 10, length.out = 100)

lasso.mod <- glmnet(trnmat, train2$SalePrice, alpha=0, lambda=lambda)

cv.lasso <- cv.glmnet(trnmat, train2$SalePrice, alpha=0, lambda=lambda, folds = 10)

bestlam.lasso <- cv.lasso$lambda.min
#bestlam.lasso

best.lasso <- glmnet(trnmat, train2$SalePrice, alpha=0, lambda=bestlam.lasso)

pred.lasso <- predict(lasso.mod, s=bestlam.lasso, newx=tstmat)

lasso_MSE <- mean((test2$SalePrice - pred.lasso)^2)

par(mfrow=c(1,2))
plot(lasso.mod, "lambda", label = TRUE)
plot(cv.lasso)

```

# Table of Test MSE Values

```{r mse, echo=FALSE}
MSE_table <- data.frame(Model = c("Decision Tree", "Random Forest", "Boosted Forest", "LASSO"), 
                        MSE = c(dt_MSE, rf_MSE, boost_MSE, lasso_MSE))

kable((MSE_table), caption = "Test MSE Values for Different Models", digits = 4)
```

# Outside Exploration

\begin{itemize}
    \item[--] Wanted to attempt to use best model to predict sale prices in Northern Colorado
    \item[--] Worked with co-owner of Deluxe Homes LLC
    \item[--] Some issues were discovered:
        \begin{itemize}
            \item[--] Using models:
                \begin{itemize}
                    \item[--] Overall Quality and Neighborhood are significant
                    \item[--] Quality is subjective
                    \item[--] Neighborhood is very different from Ames, Iowa
                \end{itemize}
            \item[--] Additional aspects not considered:
                \begin{itemize}
                    \item[--] Supply and price of building materials can change
                    \item[--] Soil quality will impact price of foundation
                \end{itemize}
        \end{itemize}
    \item[--] Data snapshot in time
\end{itemize}


# References

Ames, Iowa: Alternative to the Boston Housing Data as an ... http://jse.amstat.org/v19n3/decock.pdf. 

“Convert Character to Factor in R: Vector, Data Frame Columns &amp; Variable.” Statistics Globe, 14 June 2021, https://statisticsglobe.com/convert-character-to-factor-in-r. 

“House Prices - Advanced Regression Techniques.” Kaggle, https://www.kaggle.com/c/house-prices-advanced-regression-techniques.

Sprecher, Stu. “Deluxe Homes LLC Housing Prices.” 1 Dec. 2021. 